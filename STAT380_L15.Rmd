---
title: "Feature Selection Part 2"
output: html_document
date: "March 25, 2024"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Front Matter
```{r}
#Remove all objects from Environment
remove(list = ls())

#Load Packages
library(tidyverse)
library(glmnet) #To perform LASSO and Ridge
library(ISLR2) #For Credit Dataset

#Load data
data(Credit)
```

## Example - Applying Shrinkage Methods in R
#### Part a - Preparing the data for glmnet
```{r}
#Create input matrix and response vector
Xmat <- model.matrix(Balance ~ ., data = Credit)[ , -1]
yvec <- Credit$Balance

#Perform 80/20 training/validation split using seed of 1
set.seed(1)
trainInd <- sample(1:nrow(Credit), floor(0.8 * nrow(Credit)))
set.seed(NULL)

#Split Xmat and yvec separately
XmatTrain <- Xmat[trainInd, ]
XmatVal <- Xmat[-trainInd, ]
yvecTrain <- yvec[trainInd]
yvecVal <- yvec[-trainInd]

```

#### Part b - Build the LASSO model and plot the coefficient paths
```{r}
#Fit the LASSO model (let R choose lambda sequence)
lassoModel <- glmnet(x = XmatTrain, 
                     y = yvecTrain,
                     family = "gaussian",
                     alpha = 1,
                     lambda = NULL,
                     standardize = TRUE)

#Create a plot of the coefficient paths
plot(lassoModel, xvar = "lambda", label = TRUE)
```

#### Part c - Build the Ridge model and plot the coefficient paths
```{r}
#Fit the Ridge model (let R choice lambda sequence)
ridgeModel <- glmnet(x = XmatTrain, y = yvecTrain,
                     family = "gaussian",
                     alpha = 0, 
                     lambda = NULL,
                     standardize = TRUE)

#Create a plot of the coefficient paths
plot(ridgeModel, xvar = "lambda", label = TRUE)
```

#### Part d - For the LASSO models Extract 44th value of lambda and associated coefficients
```{r}
#Extract the 44th value of lambda
lassoModel$lambda[44]

#Calculate the log of the 44th value of lambda (doing this since the plot of coefficient paths used log(lambda))
log(lassoModel$lambda[44])

#Obtain coefficients associated with 44th value of lambda
predict(lassoModel, s = lassoModel$lambda[44], type = "coefficients")

#Add vertical line at 44th lambda on coefficients plot
plot(lassoModel, xvar = "lambda", label = TRUE)
abline(v = log(lassoModel$lambda[44]), col = "red")
```

#### Part e - Pick Optimal Value of Lambda for LASSO
```{r}
#Use 10-fold CV to pick lambda for LASSO (use seed 123)
set.seed(123)
lassoCV <- cv.glmnet(x = XmatTrain,
                     y = yvecTrain,
                     family = "gaussian",
                     alpha = 1,
                     lambda = NULL,
                     standardize = TRUE,
                     nfolds = 10)
set.seed(NULL)

#Show the LASSO CV results
plot(lassoCV)
```

#### Not on Handout - Pick Optimal Value of Lambda for Ridge
```{r}
#Use 10-fold CV to pick lambda for Ridge
set.seed(123)
ridgeCV <- cv.glmnet(x = XmatTrain, y = yvecTrain,
                    family = "gaussian",
                    alpha = 0, 
                    lambda = NULL,
                    standardize = TRUE,
                    nfolds = 10)
set.seed(NULL)

#Show the Ridge CV results
plot(ridgeCV)
```


#### Part f - Display the optimal values of lambda and the associated coefficients for LASSO
```{r}
#Display the optimal values of lambda
lassoCV$lambda.min
lassoCV$lambda.1se

#Store the coefficients associated with the optimal values
coefLamMin <- predict(lassoCV, s = lassoCV$lambda.min, type = "coefficients")
coefLam1se <- predict(lassoCV, s = lassoCV$lambda.1se, type = "coefficients")

#Create a data frame for comparing the coefficients
tempdf <- 
  data.frame(Variable = row.names(coefLamMin), 
             lamMin = as.numeric(coefLamMin), 
             lam1se = as.numeric(coefLam1se))

tempdf
```

#### Part g - Obtain Preditions for Validation Set and calculate RMSE
```{r}
#LASSO
lassoYhat <- predict(lassoCV, s = lassoCV$lambda.min,
                     newx = XmatVal)
lassoMSE <- mean((yvecVal - lassoYhat)^2)
lassoRMSE <- sqrt(lassoMSE)
lassoRMSE

#Ridge
set.seed(1)
ridgeCV <- cv.glmnet(x = XmatTrain,
                     y = yvecTrain,
                     family = "gaussian",
                     alpha = 0,
                     lambda = NULL,
                     standardize = TRUE,
                     nfolds = 10)
set.seed(NULL)

ridgeYhat <- predict(ridgeCV, s = ridgeCV$lambda.min,
                     newx = XmatVal)
ridgeMSE <- mean((yvecVal - ridgeYhat)^2)
ridgeRMSE <- sqrt(ridgeMSE)
ridgeRMSE

#Least Squares (train and validation are not the same as we used in cv.glmnet)

Credit1 <- Credit %>%
        mutate(OwnYes = ifelse(Own == "Yes", 1, 0),
               StudentYes = ifelse(Student == "Yes", 1, 0),
               MarriedYes = ifelse(Married == "Yes", 1, 0),
               RegionSouth = ifelse(Region == "South", 1, 0),
               RegionWest = ifelse(Region == "West", 1, 0))

#Scale my numeric variables
xvars <- c("Income", "Limit", "Rating", "Cards", "Age", "Education", "OwnYes", "StudentYes", "MarriedYes", "RegionSouth", "RegionWest")
Credit1[ , xvars] <- scale(Credit1[ , xvars], center = TRUE, scale = TRUE)

#Create fold values
num_folds <- 10
folds <- cut(x = 1:nrow(Credit1), breaks = num_folds, labels = FALSE)

#Randomly permute fold values
set.seed(1)
folds <- sample(folds) #randomly permute the fold values
set.seed(NULL)

#Initialize
mseVec <- rep(NA, num_folds)

#Loop for controlling the folds
for(i in 1:num_folds){
  #Training/Validation Split
  valInd <- which(folds == i)
  Validation <- Credit1[valInd, ]
  Train <- Credit1[-valInd, ]

#Build Model on Train
tempModel <- lm(Balance ~ ., data = Train)

#Obtain Predictions for Validation Set
yhat <- predict(tempModel, newdata = Validation)

#Calculate and Store MSE Values
mseVec[i] <- mean((Validation$Balance - yhat)^2)

}

#Find the RMSE value
KFoldCV <- sqrt(mean(mseVec))
KFoldCV
```

The best approach is Least Squares because it has the lowest RMSE value. 

## Lasso Code
```{r}
library(tidyverse)
library(glmnet)
library(ISLR2) #For Credit Dataset

#Load data
data(Credit)

#Create input matrix and response vector
Xmat <- model.matrix(Balance ~ ., data = Credit)[ , -1]
yvec <- Credit$Balance

#Perform 80/20 training/validation split using seed of 1
set.seed(1)
trainInd <- sample(1:nrow(Credit), floor(0.8 * nrow(Credit)))
set.seed(NULL)

#Split Xmat and yvec separately
XmatTrain <- Xmat[trainInd, ]
XmatVal <- Xmat[-trainInd, ]
yvecTrain <- yvec[trainInd]
yvecVal <- yvec[-trainInd]

#Fit the LASSO model (let R choose lambda sequence)
lassoModel <- glmnet(x = XmatTrain, 
                     y = yvecTrain,
                     family = "gaussian",
                     alpha = 1,
                     lambda = NULL,
                     standardize = TRUE)

#Create a plot of the coefficient paths
plot(lassoModel, xvar = "lambda", label = TRUE)

#Use 10-fold CV to pick lambda for LASSO (use seed 123)
set.seed(123)
lassoCV <- cv.glmnet(x = XmatTrain,
                     y = yvecTrain,
                     family = "gaussian",
                     alpha = 1,
                     lambda = NULL,
                     standardize = TRUE,
                     nfolds = 10)
set.seed(NULL)

#Display the optimal values of lambda
lassoCV$lambda.min
lassoCV$lambda.1se

#Store the coefficients associated with the optimal values
coefLamMin <- predict(lassoCV, s = lassoCV$lambda.min, type = "coefficients")
coefLam1se <- predict(lassoCV, s = lassoCV$lambda.1se, type = "coefficients")

#Create a data frame for comparing the coefficients
tempdf <- 
  data.frame(Variable = row.names(coefLamMin), 
             lamMin = as.numeric(coefLamMin), 
             lam1se = as.numeric(coefLam1se))

tempdf

#Finding RMSE
lassoYhat <- predict(lassoCV, s = lassoCV$lambda.min,
                     newx = XmatVal)
lassoMSE <- mean((yvecVal - lassoYhat)^2)
lassoRMSE <- sqrt(lassoMSE)
lassoRMSE
```

